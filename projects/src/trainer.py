# src/trainer.py

import os
from typing import Dict

import torch
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.tensorboard import SummaryWriter  # âœ… í…ì„œë³´ë“œ
from seqeval.metrics import classification_report, f1_score

from tqdm.auto import tqdm


class Trainer:
    """
    - train(): í•™ìŠµ ë£¨í”„ (train + validation F1 ì¸¡ì •)
    - evaluate(): ì£¼ì–´ì§„ split(test/val)ìœ¼ë¡œ F1 ì¸¡ì •
    - TensorBoardì— loss/F1 ê¸°ë¡
    """

    def __init__(self, model, data_module, config):
        self.model = model
        self.data = data_module
        self.config = config

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

        # config ë¶ˆëŸ¬ì˜¤ê¸°
        self.epochs = config["training"]["num_epochs"]
        self.lr = config["training"]["learning_rate"]
        self.weight_decay = config["training"]["weight_decay"]
        self.warmup_ratio = config["training"]["warmup_ratio"]
        self.output_dir = config["logging"]["output_dir"]
        os.makedirs(self.output_dir, exist_ok=True)

        # âœ… TensorBoard writer
        self.writer = SummaryWriter(log_dir=os.path.join(self.output_dir, "tb"))
        self.global_step = 0         # step ë‹¨ìœ„ scalar ê¸°ë¡ìš©
        self.current_epoch = 0       # epoch ë‹¨ìœ„ F1 ê¸°ë¡ìš©

        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay,
        )

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„
        total_steps = self.epochs * len(self.data.train_dataloader)
        warmup_steps = int(total_steps * self.warmup_ratio)

        def lr_lambda(current_step):
            if current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            return max(
                0.0,
                float(total_steps - current_step)
                / float(max(1, total_steps - warmup_steps)),
            )

        self.scheduler = LambdaLR(self.optimizer, lr_lambda)

        self.best_f1 = 0.0

    # ------------------------------------------------------
    # Train Loop
    # ------------------------------------------------------
    def train(self):
        print(f"ğŸ”¥ Training started for {self.epochs} epochs")
        for epoch in range(1, self.epochs + 1):
            self.current_epoch = epoch
            print(f"\n===== Epoch {epoch}/{self.epochs} =====")
            self.model.train()

            total_loss = 0

            for batch in tqdm(self.data.train_dataloader):
                batch = {k: v.to(self.device) for k, v in batch.items()}

                outputs = self.model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                    labels=batch["labels"],
                )

                loss = outputs["loss"]
                total_loss += loss.item()

                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), self.config["training"]["max_grad_norm"]
                )

                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

                # âœ… step ë‹¨ìœ„ loss ê¸°ë¡
                self.writer.add_scalar(
                    "Loss/step", loss.item(), self.global_step
                )
                self.global_step += 1

            avg_loss = total_loss / len(self.data.train_dataloader)
            print(f"Epoch {epoch} | Train Loss: {avg_loss:.4f}")

            # âœ… epoch ë‹¨ìœ„ loss ê¸°ë¡
            self.writer.add_scalar("Loss/epoch", avg_loss, epoch)

            # -----------------------------
            # Validation step
            # -----------------------------
            val_f1 = self.evaluate("val")   # ë‚´ë¶€ì—ì„œ F1/val ê¸°ë¡

            # save best
            if val_f1 > self.best_f1:
                self.best_f1 = val_f1
                save_path = os.path.join(self.output_dir, "best_model.pt")
                torch.save(self.model.state_dict(), save_path)
                print(f"â­ Best model updated! F1={val_f1:.4f} saved to {save_path}")

        # í•™ìŠµ ëë‚˜ë©´ flush
        self.writer.flush()

    # ------------------------------------------------------
    # Evaluate (val or test)
    # ------------------------------------------------------
    def evaluate(self, split="val"):
        self.model.eval()

        if split == "val":
            dataloader = self.data.eval_dataloader
        else:
            dataloader = self.data.test_dataloader

        preds = []
        trues = []

        label_list = self.data.label_list

        with torch.no_grad():
            for batch in tqdm(dataloader):
                batch = {k: v.to(self.device) for k, v in batch.items()}
                outputs = self.model(
                    input_ids=batch["input_ids"],
                    attention_mask=batch["attention_mask"],
                )

                logits = outputs["logits"]  # (B, L, C)
                preds_batch = torch.argmax(logits, dim=-1).cpu().numpy()
                true_batch = batch["labels"].cpu().numpy()

                for p_seq, t_seq in zip(preds_batch, true_batch):
                    pred_tags = []
                    true_tags = []

                    for p, t in zip(p_seq, t_seq):
                        if t == -100:
                            continue  # ignore padding/subword
                        pred_tags.append(label_list[p])
                        true_tags.append(label_list[t])

                    preds.append(pred_tags)
                    trues.append(true_tags)

        f1 = f1_score(trues, preds)
        print(f"[{split.upper()}] F1-score: {f1:.4f}")

        # testì¼ ê²½ìš° ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥
        if split == "test":
            print(classification_report(trues, preds))

        # âœ… TensorBoardì— F1 ê¸°ë¡
        if split == "val":
            self.writer.add_scalar("F1/val", f1, self.current_epoch)
        elif split == "test":
            # testëŠ” ë§ˆì§€ë§‰ epoch ê¸°ì¤€ìœ¼ë¡œ ê¸°ë¡
            self.writer.add_scalar("F1/test", f1, self.current_epoch)

        return f1